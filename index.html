<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">

  <meta name="description" content="">
  <meta name="keywords" content="NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>High-Fidelity 3D Head Avatars Reconstruction through Spatially-Varying Expression Conditioned Neural Radiance Field (AAAI2024)</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./css/bulma.min.css">
  <link rel="stylesheet" href="./css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./css/bulma-slider.min.css">
  <link rel="stylesheet" href="./css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./js/fontawesome.all.min.js"></script>
  <script src="./js/bulma-carousel.min.js"></script>
  <script src="./js/bulma-slider.min.js"></script>
  <script src="./js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">High-Fidelity 3D Head Avatars Reconstruction through Spatially-Varying Expression Conditioned Neural Radiance Field
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block"> <a href="https://github.com/minghanqin">Minghan Qin</a><sup>*</sup>,</span>
              <span class="author-block"> Yifan Liu<sup>*</sup>,</span>
              <span class="author-block"> Yuelang Xu,</span>
              <span class="author-block"> Xiaochen Zhao,</span>
              <span class="author-block"> Yebin Liu<sup>#</sup>,</span>
              <span class="author-block"> Haoqian Wang<sup>#</sup></span>
            </div>

            <div class="is-size-5">
              * indicates equal contribution, # means Co-corresponding author
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Tsinghua University</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2310.06275"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=dPgAa-6Kt8Q"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/minghanqin/AvatarSVE"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>One crucial aspect of 3D head avatar reconstruction lies in the details of facial expressions. 
              Although recent NeRF-based photo-realistic 3D head avatar methods achieve high-quality avatar rendering, 
              they still encounter challenges retaining intricate facial expression details because they overlook the 
              potential of specific expression variations at different spatial positions when conditioning the radiance 
              field. Motivated by this observation, we introduce a novel Spatially-Varying Expression (SVE) conditioning.
              The SVE can be obtained by a simple MLP-based generation network, encompassing both spatial positional 
              features and global expression information. Benefiting from rich and diverse information of the SVE at 
              different positions, the proposed SVE-conditioned neural radiance field can deal with intricate facial 
              expressions and achieve realistic rendering and geometry details of high-fidelity 3D head avatars. 
              Additionally, to further elevate the geometric and rendering quality, we introduce a new coarse-to-fine 
              training strategy, including a geometry initialization strategy at the coarse stage and an adaptive 
              importance sampling strategy at the fine stage. Extensive experiments indicate that our method outperforms
              other state-of-the-art (SOTA) methods in rendering and geometry quality on mobile phone-collected and 
              public datasets.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/watch?v=dPgAa-6Kt8Q" title="YouTube video player" frameborder="0"
              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
              allowfullscreen></iframe>
          </div>
          <div class="content has-text-justified">
            <p>If the video does not play, please click <a href="https://www.youtube.com/watch?v=dPgAa-6Kt8Q">here</a> to watch it.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="method">
    <div class="container is-max-desktop content">
      <h2 class="title">Method</h2>
      <img src="./content/pipeline.png" class="center" />
      <p>Given a portrait video, We first track the global expression parameters &varepsilon; using 3DMM. 
        After the pre-processing, given the sampled 3D points p<sub>o</sub> in observation space, we apply the 
        generation network G to extend the global expression parameters &varepsilon; with the spatial positional 
        features of each position p<sub>o</sub> in 3D space. Then, through a deformation network D, we 
        transform p<sub>o</sub> from the observation space to the p<sub>c</sub> in the canonical space 
        conditioned on &varepsilon;'. Subsequently, we use &varepsilon;' conditioned NeuS to predict 
        the SDF values and color c corresponding to p<sub>c</sub>. Finally, we obtain the rendered 
        RGB image and normal using volumetric rendering.</p>
    </div>
  </section>


  <section class="section" id="result">
    <div class="container is-max-desktop content">
      <h2 class="title">Result</h2>
      <h3 class="title">Self-Reenactment</h2>
        <div class="columns is-max-desktop content">
        <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="auto" muted="">
          <source src="content/self-reenactment1.mp4" type="video/mp4">
        </video>
        </div>
        <div class="columns is-max-desktop content">
        <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="auto" muted="">
          <source src="content/self-reenactment2.mp4" type="video/mp4">
        </video>
        </div>

        <div class="columns is-max-desktop content">
        <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="auto" muted="">
          <source src="content/self-reenactment3.mp4" type="video/mp4">
        </video>
        </div>
        <div class="columns is-max-desktop content">
        <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="auto" muted="">
          <source src="content/self-reenactment4.mp4" type="video/mp4">
        </video>
        </div>
      <h3 class="title">Cross-Identity Reenactment</h2>
        <div class="columns is-max-desktop content">
          <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="auto" muted="">
            <source src="content/cross-identity reenactment1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="columns is-max-desktop content">
          <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="auto" muted="">
            <source src="content/cross-identity reenactment2.mp4" type="video/mp4">
          </video>
        </div>
          
    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{qin2023high,
    title={High-Fidelity 3D Head Avatars Reconstruction through Spatially-Varying Expression Conditioned Neural Radiance Field},
    author={Qin, Minghan and Liu, Yifan and Xu, Yuelang and Zhao, Xiaochen and Liu, Yebin and Wang, Haoqian},
    journal={arXiv preprint arXiv:2310.06275},
    year={2023}
  }</code></pre>
    </div>
  </section>

</body>

</html>